name: Enhanced Test Suite (Grade A)

on:
  push:
    branches: [ main, develop, feature/*, hotfix/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  test-matrix:
    name: Test Matrix - Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.11', '3.12', '3.13']
        include:
          # Extended testing for primary configuration
          - os: ubuntu-latest
            python-version: '3.13'
            run-comprehensive: true
            run-performance: true

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better coverage

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential

    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-latest'
      run: |
        brew install libomp

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-xdist pytest-mock pytest-benchmark
        pip install pytest-timeout pytest-profiling pytest-rerunfailures
        pip install coverage[toml] codecov

    - name: Validate Python syntax
      run: |
        python -m py_compile agents/**/*.py
        python -m py_compile model_pack/**/*.py

    - name: Run unit tests
      if: matrix.run-comprehensive != true
      run: |
        pytest tests/ \
          -m "unit or integration" \
          --cov=agents --cov=model_pack \
          --cov-report=xml --cov-report=term-missing \
          --junit-xml=test-results-unit.xml \
          --durations=10 \
          -v

    - name: Run comprehensive test suite
      if: matrix.run-comprehensive == true
      run: |
        pytest tests/ \
          --cov=agents --cov=model_pack \
          --cov-report=xml --cov-report=html \
          --cov-report=term-missing \
          --junit-xml=test-results-comprehensive.xml \
          --durations=10 \
          --benchmark-only \
          -v

    - name: Run performance tests
      if: matrix.run-performance == true
      run: |
        pytest tests/ \
          -m "performance or stress" \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          -v

    - name: Generate test report
      run: |
        python -c "
        import json
        import xml.etree.ElementTree as ET

        # Parse test results
        try:
            tree = ET.parse('test-results-comprehensive.xml')
            root = tree.getroot()
            tests = root.attrib.get('tests', 0)
            failures = root.attrib.get('failures', 0)
            errors = root.attrib.get('errors', 0)
            skipped = root.attrib.get('skipped', 0)

            print(f'Tests: {tests}, Failures: {failures}, Errors: {errors}, Skipped: {skipped}')

            # Generate summary
            summary = {
                'total_tests': int(tests),
                'failures': int(failures),
                'errors': int(errors),
                'skipped': int(skipped),
                'success_rate': (int(tests) - int(failures) - int(errors)) / int(tests) * 100 if int(tests) > 0 else 0
            }

            with open('test-summary.json', 'w') as f:
                json.dump(summary, f, indent=2)

        except Exception as e:
            print(f'Could not parse test results: {e}')
        "

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: |
          test-results*.xml
          test-summary.json
          coverage.xml
          htmlcov/
          benchmark-results.json
        retention-days: 30

    - name: Upload performance benchmarks
      uses: actions/upload-artifact@v3
      if: matrix.run-performance == true
      with:
        name: performance-benchmarks-${{ matrix.os }}-py${{ matrix.python-version }}
        path: benchmark-results.json
        retention-days: 30

  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    needs: test-matrix
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download test artifacts
      uses: actions/download-artifact@v3
      with:
        pattern: test-results-*
        merge-multiple: true

    - name: Evaluate quality gates
      run: |
        python -c "
        import json
        import sys
        from pathlib import Path

        # Quality gate thresholds
        MIN_COVERAGE = 90
        MIN_SUCCESS_RATE = 95
        MAX_FAILED_TESTS = 5

        gates_passed = []
        gates_failed = []

        # Check coverage
        try:
            with open('coverage.xml') as f:
                content = f.read()
                # Extract coverage percentage (simplified)
                import re
                coverage_match = re.search(r'line-rate=\"([0-9.]+)\"', content)
                if coverage_match:
                    coverage = float(coverage_match.group(1)) * 100
                    if coverage >= MIN_COVERAGE:
                        gates_passed.append(f'Coverage: {coverage:.1f}% (‚â•{MIN_COVERAGE}%)')
                    else:
                        gates_failed.append(f'Coverage: {coverage:.1f}% (<{MIN_COVERAGE}%)')
        except Exception as e:
            gates_failed.append(f'Coverage check failed: {e}')

        # Check test success rate
        try:
            with open('test-summary.json') as f:
                summary = json.load(f)
                success_rate = summary.get('success_rate', 0)
                failures = summary.get('failures', 0) + summary.get('errors', 0)

                if success_rate >= MIN_SUCCESS_RATE and failures <= MAX_FAILED_TESTS:
                    gates_passed.append(f'Success rate: {success_rate:.1f}% (‚â•{MIN_SUCCESS_RATE}%), Failures: {failures} (‚â§{MAX_FAILED_TESTS})')
                else:
                    gates_failed.append(f'Success rate: {success_rate:.1f}% (<{MIN_SUCCESS_RATE}%) or Failures: {failures} (>{MAX_FAILED_TESTS})')
        except Exception as e:
            gates_failed.append(f'Test summary check failed: {e}')

        # Print results
        print('\\n=== QUALITY GATES RESULTS ===')
        for gate in gates_passed:
            print(f'‚úÖ {gate}')
        for gate in gates_failed:
            print(f'‚ùå {gate}')

        # Determine overall result
        if gates_failed:
            print(f'\\nüö® QUALITY GATES FAILED: {len(gates_failed)} gate(s) failed')
            sys.exit(1)
        else:
            print(f'\\n‚úÖ ALL QUALITY GATES PASSED: {len(gates_passed)} gate(s) passed')
        "

    - name: Security scan
      run: |
        pip install bandit safety
        bandit -r agents/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 30

  performance-regression:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: test-matrix
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Download performance benchmarks
      uses: actions/download-artifact@v3
      with:
        name: performance-benchmarks-ubuntu-latest-py3.13
        path: current-benchmarks

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.13'

    - name: Install dependencies
      run: |
        pip install pytest pytest-benchmark
        pip install -r requirements.txt

    - name: Run baseline benchmarks
      run: |
        git checkout HEAD~1  # Compare with previous commit
        pytest tests/ -m "performance" --benchmark-only --benchmark-json=baseline-benchmarks.json

    - name: Compare performance
      run: |
        python -c "
        import json
        import sys

        try:
            with open('baseline-benchmarks.json') as f:
                baseline = json.load(f)
            with open('current-benchmarks/benchmark-results.json') as f:
                current = json.load(f)

            # Compare key metrics
            regression_detected = False

            if 'benchmarks' in baseline and 'benchmarks' in current:
                for bench in current['benchmarks']:
                    name = bench['name']
                    current_time = bench['stats']['mean']

                    # Find baseline
                    baseline_time = None
                    for b in baseline['benchmarks']:
                        if b['name'] == name:
                            baseline_time = b['stats']['mean']
                            break

                    if baseline_time:
                        change_percent = ((current_time - baseline_time) / baseline_time) * 100
                        if change_percent > 20:  # 20% regression threshold
                            print(f'üö® PERFORMANCE REGRESSION: {name}')
                            print(f'   Baseline: {baseline_time:.6f}s')
                            print(f'   Current:  {current_time:.6f}s')
                            print(f'   Change:   {change_percent:+.1f}%')
                            regression_detected = True
                        else:
                            print(f'‚úÖ {name}: {change_percent:+.1f}% (within limits)')

            if regression_detected:
                print('\\nüö® PERFORMANCE REGRESSION DETECTED')
                sys.exit(1)
            else:
                print('\\n‚úÖ NO PERFORMANCE REGRESSION')

        except Exception as e:
            print(f'Performance comparison failed: {e}')
            # Don't fail the build for comparison issues
        "

  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [test-matrix, quality-gates]
    if: always()

    steps:
    - name: Notify Slack on Success
      if: needs.quality-gates.result == 'success'
      run: |
        echo "‚úÖ All tests passed and quality gates cleared!"
        # Add Slack notification here if webhook is configured

    - name: Notify Slack on Failure
      if: needs.quality-gates.result == 'failure'
      run: |
        echo "‚ùå Tests failed or quality gates not met!"
        # Add Slack notification here if webhook is configured