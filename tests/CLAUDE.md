# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with the testing framework and quality assurance components.

## ðŸŽ¯ Quick Start for Testing

```bash
# Navigate to tests directory
cd tests/

# Run comprehensive test suite (RECOMMENDED)
python -m pytest test_agent_system.py -v

# Run all tests with coverage
pytest --cov=../agents --cov=../model_pack --cov-report=html

# Quick validation test
python ../project_management/TOOLS_AND_CONFIG/test_agents.py

# Full system quality assurance
python ../project_management/QUALITY_ASSURANCE/test_fixed_system.py
```

## Testing Overview

This directory contains comprehensive test suite for Script Ohio 2.0, implementing pytest-based testing for agent system, machine learning models, data quality, and integration components. This ensures platform reliability and maintains high code quality standards.

### ðŸ§ª Test Architecture

The test suite follows pytest best practices with:

- **Unit Tests**: Individual component testing with mocked dependencies
- **Integration Tests**: Multi-component interaction testing
- **End-to-End Tests**: Complete workflow validation
- **Performance Tests**: Response time and resource usage validation
- **Quality Assurance**: Data quality and model validation

## Test Categories

### **1. Agent System Tests** (`test_agent_system.py`)
- **Purpose**: Test intelligent agent architecture and orchestration
- **Coverage**: Context Manager, Analytics Orchestrator, Agent Framework
- **Key Tests**:
  - User role detection and context optimization
  - Agent request routing and response synthesis
  - Permission-based access control
  - Performance monitoring and caching
- **Validation**: Role-based token reduction, response time <2s

### **2. Model Validation Tests** (`test_model_system.py`)
- **Purpose**: Validate ML model integration and predictions
- **Coverage**: Model loading, prediction pipeline, performance metrics
- **Key Tests**:
  - Model file loading and validation
  - Feature preprocessing consistency
  - Prediction accuracy and calibration
  - Ensemble model performance
- **Validation**: Model accuracy >40%, MAE <18 points for margin predictions

### **3. Data Quality Tests** (`test_data_quality.py`)
- **Purpose**: Ensure data integrity and consistency across datasets
- **Coverage**: Starter pack data, model pack training data, 2025 integration
- **Key Tests**:
  - Data completeness and missing value handling
  - Column consistency and naming conventions
  - Opponent adjustment calculations
  - Temporal data validation (2016-2025)
- **Validation**: 100% data completeness, consistent feature calculations

### **4. Integration Tests** (`test_integration.py`)
- **Purpose**: Test complete workflows and system integration
- **Coverage**: End-to-end user journeys, API integration
- **Key Tests**:
  - Complete analytics request workflows
  - Model predictions with live data
  - Agent system and model pack integration
  - Error handling and graceful degradation
- **Validation**: Workflow success rate >95%

## Testing Framework

### **Pytest Configuration**
```python
# pytest.ini or pyproject.toml configuration
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    -v
    --tb=short
    --strict-markers
    --cov=agents
    --cov=model_pack
    --cov-report=term-missing
    --cov-report=html
```

### **Test Structure**
```python
import pytest
from unittest.mock import Mock, patch
from agents.analytics_orchestrator import AnalyticsOrchestrator, AnalyticsRequest

class TestAnalyticsOrchestrator:
    def setup_method(self):
        """Setup for each test method"""
        self.orchestrator = AnalyticsOrchestrator()

    def test_user_role_detection(self):
        """Test automatic user role detection"""
        request = AnalyticsRequest(
            user_id="test_user",
            query="basic analytics question",
            query_type="learning",
            parameters={},
            context_hints={}
        )

        response = self.orchestrator.process_analytics_request(request)

        assert response.status == "success"
        assert "role_detected" in response.metadata
```

### **Test Fixtures**
```python
@pytest.fixture
def sample_analytics_request():
    """Provide sample request for testing"""
    return AnalyticsRequest(
        user_id="test_user_001",
        query="Analyze team performance",
        query_type="analysis",
        parameters={"teams": ["Ohio State", "Michigan"]},
        context_hints={"skill_level": "intermediate"}
    )

@pytest.fixture
def mock_model_predictions():
    """Mock model predictions for testing"""
    return {
        "win_probability": 0.65,
        "predicted_margin": 7.2,
        "confidence_interval": (2.1, 12.3)
    }
```

## Test Data and Mocking

### **Test Data Management**
- **Location**: `tests/fixtures/` directory for test data files
- **Samples**: Small, representative datasets for unit testing
- **Isolation**: Test data isolated from production data
- **Version Control**: Test data committed for reproducible tests

### **Mock Strategy**
```python
# Mock external dependencies
@patch('agents.model_execution_engine.joblib.load')
def test_model_prediction_with_mock(mock_load):
    """Test model prediction with mocked model loading"""
    mock_model = Mock()
    mock_model.predict.return_value = [0.65]  # Win probability
    mock_load.return_value = mock_model

    from agents.model_execution_engine import ModelExecutionEngine
    engine = ModelExecutionEngine()
    result = engine.predict_win_probability(["Ohio State", "Michigan"])

    assert result["win_probability"] == 0.65
    mock_load.assert_called_once()
```

### **Database Mocking**
```python
@pytest.fixture
def mock_database_connection():
    """Mock database connections for testing"""
    with patch('pandas.read_csv') as mock_read:
        mock_read.return_value = pd.DataFrame({
            'team': ['Ohio State', 'Michigan'],
            'conference': ['Big Ten', 'Big Ten'],
            'elo': [85.2, 82.1]
        })
        yield mock_read
```

## Performance Testing

### **Response Time Validation**
```python
import time

def test_orchestrator_response_time_performance():
    """Test that orchestrator responds within 2 seconds"""
    orchestrator = AnalyticsOrchestrator()
    request = create_complex_request()  # Helper function

    start_time = time.time()
    response = orchestrator.process_analytics_request(request)
    end_time = time.time()

    response_time = end_time - start_time
    assert response_time < 2.0, f"Response time {response_time}s exceeds 2s limit"
    assert response.status == "success"
```

### **Memory Usage Testing**
```python
import psutil
import os

def test_memory_usage_within_limits():
    """Test that agent operations don't exceed memory limits"""
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss

    # Perform memory-intensive operation
    orchestrator = AnalyticsOrchestrator()
    for _ in range(100):
        orchestrator.process_analytics_request(create_sample_request())

    final_memory = process.memory_info().rss
    memory_increase = (final_memory - initial_memory) / 1024 / 1024  # MB

    assert memory_increase < 100, f"Memory increase {memory_increase}MB exceeds 100MB limit"
```

## Quality Assurance Testing

### **Data Quality Validation**
```python
def test_training_data_quality():
    """Test that training data meets quality standards"""
    from model_pack.data_loader import load_training_data

    df = load_training_data()

    # Check for missing values
    assert df.isnull().sum().sum() == 0, "Training data should have no missing values"

    # Check for required columns
    required_columns = ['home_team', 'away_team', 'season', 'week']
    for col in required_columns:
        assert col in df.columns, f"Required column {col} missing from training data"

    # Check data ranges
    assert df['season'].min() >= 2016, "Training data should start from 2016"
    assert df['season'].max() <= 2025, "Training data should end by 2025"
```

### **Model Performance Validation**
```python
def test_model_performance_meets_benchmarks():
    """Test that models meet established performance benchmarks"""
    from agents.model_execution_engine import ModelExecutionEngine

    engine = ModelExecutionEngine()
    test_data = load_test_games_2025()

    predictions = []
    actuals = []

    for _, game in test_data.iterrows():
        pred = engine.predict_win_probability([game['home_team'], game['away_team']])
        predictions.append(pred['win_probability'])
        actuals.append(1 if game['home_points'] > game['away_points'] else 0)

    accuracy = calculate_accuracy(predictions, actuals)
    assert accuracy >= 0.40, f"Model accuracy {accuracy} below 40% benchmark"
```

## ðŸš€ Development Commands

### Environment Setup
```bash
# Testing dependencies (Python 3.13+)
pip install pytest pytest-cov pytest-mock pytest-asyncio

# Core dependencies for testing
pip install pandas numpy scikit-learn

# Navigate to tests directory
cd tests/
```

### Running Tests
```bash
# Run comprehensive test suite (RECOMMENDED)
python -m pytest ../tests/test_agent_system.py -v

# Run all tests
pytest ../tests/

# Run specific test components
python -m pytest ../tests/test_agent_system.py::TestContextManager -v
python -m pytest ../tests/test_agent_system.py::TestAnalyticsOrchestrator -v

# Run with coverage reporting
pytest --cov=../agents --cov=../model_pack ../tests/

# Run performance tests
pytest -m performance ../tests/

# Run integration tests only
pytest -m integration ../tests/
```

### Additional Test Commands
```bash
# Test model pack specifically
python -m pytest ../tests/test_model_pack_comprehensive.py -v

# Test week12 agents
python -m pytest ../tests/test_week12_agents_comprehensive.py -v

# Test model execution engine
python -m pytest ../tests/test_model_execution_engine_comprehensive.py -v

# Quick validation test
python ../project_management/TOOLS_AND_CONFIG/test_agents.py

# Full system quality assurance
python ../project_management/QUALITY_ASSURANCE/test_fixed_system.py
```

### Test Configuration and Setup
```bash
# Development environment setup
export TESTING=true
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# Run with verbose output and detailed reporting
pytest -v --tb=short ../tests/

# Run with specific markers
pytest -m "not slow" ../tests/  # Skip slow tests
pytest -m "unit and agent_system" ../tests/  # Run only unit tests for agent system
```

### **Test Configuration**
```bash
# Development environment
export TESTING=true
export DATABASE_URL=sqlite:///test.db

# Run with verbose output
pytest -v tests/

# Run with specific markers
pytest -m "not slow" tests/  # Skip slow tests
pytest -m "unit and agent_system" tests/  # Run only unit tests for agent system
```

### **Continuous Integration**
```yaml
# .github/workflows/test.yml
name: Test Suite
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.13'
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    - name: Run tests
      run: |
        pytest --cov=agents --cov=model_pack --cov-report=xml
    - name: Upload coverage
      uses: codecov/codecov-action@v1
```

## Test Reporting and Metrics

### **Coverage Requirements**
- **Minimum Coverage**: 90% for all production code
- **Critical Path Coverage**: 100% for core agent orchestration
- **Model Integration**: 95% coverage for model execution engine
- **Data Quality**: 90% coverage for data loading and validation

### **Performance Benchmarks**
- **Response Time**: <2 seconds for all agent operations
- **Memory Usage**: <100MB increase for normal operations
- **Cache Hit Rate**: >80% for repeated requests
- **Error Rate**: <1% for all operations

### **Quality Metrics**
- **Data Completeness**: 100% complete training data
- **Model Accuracy**: >40% win probability accuracy
- **Feature Consistency**: 100% consistent feature calculations
- **Integration Success**: >95% end-to-end workflow success

## Troubleshooting Common Test Issues

### **Import Errors**
```bash
# Fix Python path issues
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
pytest tests/

# Or use conftest.py to set up path
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
```

### **Mock Failures**
```python
# Ensure proper mock patching
# Wrong: patch('module.Class') when imported differently
@patch('agents.analytics_orchestrator.AnalyticsOrchestrator')  # Correct

# Use patch.object for instance methods
mock_orchestrator = Mock()
with patch.object(mock_orchestrator, 'process_request', return_value=mock_response):
    # Test implementation
```

### **Async Testing**
```python
# For async agent methods
import pytest_asyncio

@pytest.mark.asyncio
async def test_async_agent_method():
    agent = AsyncAgent()
    result = await agent.process_async_request(request)
    assert result.status == "success"
```

## Test Environment Setup

### **Development Testing**
```bash
# Set up test environment
python -m venv test_env
source test_env/bin/activate
pip install -r requirements.txt
pip install pytest pytest-cov pytest-mock

# Run tests locally
pytest tests/ -v
```

### **Docker Testing**
```dockerfile
# Dockerfile.test
FROM python:3.13

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .

# Install test dependencies
RUN pip install pytest pytest-cov

# Run tests
CMD ["pytest", "tests/", "-v"]
```

## ðŸ”— Integration with Agent System Testing

### Agent System Test Coverage
```python
# Test core agent orchestration
def test_analytics_orchestrator_integration():
    """Test complete agent system workflow"""
    from agents.analytics_orchestrator import AnalyticsOrchestrator

    orchestrator = AnalyticsOrchestrator()

    # Test different user roles
    test_requests = [
        ("analyst", "Explain EPA simply", "learning"),
        ("data_scientist", "Compare model performance", "analysis"),
        ("production", "Predict game outcome", "prediction")
    ]

    for role, query, query_type in test_requests:
        request = AnalyticsRequest(
            user_id=f"test_{role}",
            query=query,
            query_type=query_type,
            parameters={},
            context_hints={"role": role}
        )

        response = orchestrator.process_analytics_request(request)

        # Validate response
        assert response.status == "success"
        assert "execution_time" in response.metadata
        assert response.metadata["execution_time"] < 2.0  # SLA requirement
```

### Model Integration Testing
```python
# Test ML model integration with agents
def test_model_execution_engine_integration():
    """Test model predictions through agent system"""
    from agents.model_execution_engine import ModelExecutionEngine

    engine = ModelExecutionEngine()

    # Test prediction pipeline
    test_games = [
        ("Ohio State", "Michigan"),
        ("Alabama", "Georgia"),
        ("Texas", "Oklahoma")
    ]

    for home_team, away_team in test_games:
        result = engine.predict_win_probability([home_team, away_team])

        # Validate prediction format
        assert "win_probability" in result
        assert "confidence_interval" in result
        assert 0.0 <= result["win_probability"] <= 1.0
```

## ðŸš€ Advanced Testing Strategies

### Performance Testing Framework
```python
import time
import pytest
from concurrent.futures import ThreadPoolExecutor

class TestPerformanceRequirements:
    """Comprehensive performance testing for agent system"""

    @pytest.mark.performance
    def test_concurrent_request_handling(self):
        """Test system handles concurrent requests"""
        from agents.analytics_orchestrator import AnalyticsOrchestrator

        orchestrator = AnalyticsOrchestrator()

        def make_request(request_id):
            """Make single test request"""
            request = AnalyticsRequest(
                user_id=f"perf_test_{request_id}",
                query="Test query for performance",
                query_type="learning",
                parameters={},
                context_hints={}
            )
            start_time = time.time()
            response = orchestrator.process_analytics_request(request)
            return time.time() - start_time, response.status

        # Test with 10 concurrent requests
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(make_request, i) for i in range(10)]
            results = [future.result() for future in futures]

        # Validate performance requirements
        response_times = [result[0] for result in results]
        success_rate = sum(1 for _, status in results if status == "success") / len(results)

        assert success_rate >= 0.95, f"Success rate {success_rate} below 95%"
        assert max(response_times) < 3.0, f"Max response time {max(response_times)}s exceeds 3s"
        assert sum(response_times) / len(response_times) < 2.0, "Average response time exceeds 2s"

    @pytest.mark.performance
    def test_memory_usage_stability(self):
        """Test system maintains stable memory usage"""
        import psutil
        import os

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss

        # Execute multiple requests
        orchestrator = AnalyticsOrchestrator()
        for i in range(100):
            request = AnalyticsRequest(
                user_id=f"memory_test_{i}",
                query=f"Test query {i}",
                query_type="learning",
                parameters={},
                context_hints={}
            )
            orchestrator.process_analytics_request(request)

        final_memory = process.memory_info().rss
        memory_increase = (final_memory - initial_memory) / 1024 / 1024  # MB

        assert memory_increase < 50, f"Memory increased by {memory_increase}MB"
```

### Integration Testing with CFBD API
```python
@pytest.mark.integration
def test_cfbd_api_integration():
    """Test CFBD API integration with error handling"""
    from unittest.mock import patch, Mock

    # Mock CFBD API responses
    with patch('cfbd.GamesApi.get_games') as mock_games:
        mock_games.return_value = [
            Mock(to_dict=lambda: {
                'id': 401485131,
                'season': 2025,
                'week': 12,
                'home_team': 'Ohio State',
                'away_team': 'Michigan',
                'home_points': 31,
                'away_points': 24
            })
        ]

        # Test API integration through agent system
        from agents.analytics_orchestrator import AnalyticsOrchestrator
        orchestrator = AnalyticsOrchestrator()

        request = AnalyticsRequest(
            user_id="api_test",
            query="Get recent Ohio State games",
            query_type="data_fetch",
            parameters={"team": "Ohio State", "year": 2025},
            context_hints={}
        )

        response = orchestrator.process_analytics_request(request)
        assert response.status == "success"
        assert "games" in response.data
```

## ðŸ“Š Test Reporting and Metrics

### Automated Test Reporting
```python
# Generate comprehensive test report
def generate_test_report():
    """Generate detailed test execution report"""

    import subprocess
    import json
    from datetime import datetime

    # Run tests with JSON output
    result = subprocess.run([
        'pytest',
        '--json-report-file=test_report.json',
        '--cov=../agents',
        '--cov=../model_pack',
        '--cov-report=json',
        'test_agent_system.py'
    ], capture_output=True, text=True)

    # Parse results
    with open('test_report.json', 'r') as f:
        test_results = json.load(f)

    # Generate summary
    report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_tests': test_results['summary']['total'],
            'passed': test_results['summary']['passed'],
            'failed': test_results['summary']['failed'],
            'skipped': test_results['summary']['skipped'],
            'success_rate': test_results['summary']['passed'] / test_results['summary']['total']
        },
        'coverage': {
            'agents': 'coverage.json',  # Parse coverage files
            'model_pack': 'coverage.json'
        },
        'performance': {
            'total_duration': test_results['summary']['duration']
        }
    }

    return report

# Automated quality gates
def run_quality_gates():
    """Execute automated quality checks"""

    quality_report = {
        'timestamp': datetime.now().isoformat(),
        'checks': {}
    }

    # Check test coverage
    coverage_result = subprocess.run([
        'pytest', '--cov=../agents', '--cov-fail-under=90', 'test_agent_system.py'
    ], capture_output=True)

    quality_report['checks']['coverage'] = {
        'passed': coverage_result.returncode == 0,
        'threshold': 90,
        'message': 'Coverage meets 90% threshold' if coverage_result.returncode == 0 else 'Coverage below threshold'
    }

    # Check performance benchmarks
    performance_result = subprocess.run([
        'pytest', '-m', 'performance', 'test_agent_system.py'
    ], capture_output=True)

    quality_report['checks']['performance'] = {
        'passed': performance_result.returncode == 0,
        'message': 'Performance tests passed' if performance_result.returncode == 0 else 'Performance tests failed'
    }

    # Overall quality status
    quality_report['overall_status'] = all(
        check['passed'] for check in quality_report['checks'].values()
    )

    return quality_report
```

## ðŸ”§ Test Environment Management

### Test Configuration
```python
# conftest.py - Shared test configuration
import pytest
import tempfile
import os
from pathlib import Path

@pytest.fixture(scope="session")
def test_data_dir():
    """Provide temporary directory for test data"""
    with tempfile.TemporaryDirectory() as temp_dir:
        yield Path(temp_dir)

@pytest.fixture(scope="session")
def mock_model_files(test_data_dir):
    """Create mock model files for testing"""
    import joblib
    import numpy as np

    # Create mock models
    models = {
        'ridge_model_2025.joblib': {'type': 'ridge', 'accuracy': 0.72},
        'xgb_home_win_model_2025.pkl': {'type': 'xgboost', 'accuracy': 0.75},
        'fastai_home_win_model_2025.pkl': {'type': 'fastai', 'accuracy': 0.73}
    }

    model_files = {}
    for filename, model_data in models.items():
        model_path = test_data_dir / filename

        # Create simple mock model
        if filename.endswith('.joblib'):
            mock_model = type('MockModel', (), {
                'predict': lambda x: np.random.random(len(x)),
                'predict_proba': lambda x: np.random.random((len(x), 2))
            })()
            joblib.dump(mock_model, model_path)
        else:
            # Create empty pickle file for non-joblib models
            model_path.touch()

        model_files[filename] = model_path

    return model_files

@pytest.fixture
def mock_cfbd_data():
    """Provide mock CFBD API data"""
    return {
        'games': [
            {
                'id': 401485131,
                'season': 2025,
                'week': 12,
                'home_team': 'Ohio State',
                'away_team': 'Michigan',
                'home_points': 31,
                'away_points': 24
            }
        ],
        'teams': [
            {'school': 'Ohio State', 'conference': 'Big Ten'},
            {'school': 'Michigan', 'conference': 'Big Ten'}
        ]
    }
```

### Environment-Specific Testing
```bash
# Development testing
export TESTING_ENV=development
pytest tests/ -v --tb=short

# CI/CD testing
export TESTING_ENV=ci
pytest tests/ -v --tb=short --junitxml=test_results.xml

# Performance testing
export TESTING_ENV=performance
pytest -m performance tests/ --benchmark-only

# Integration testing
export TESTING_ENV=integration
pytest -m integration tests/ -v
```

## ðŸ“ˆ Continuous Integration Integration

### GitHub Actions Workflow
```yaml
# .github/workflows/test.yml
name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.13]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run syntax validation
      run: |
        find . -name "*.py" -exec python3 -m py_compile {} \;

    - name: Run tests with coverage
      run: |
        cd tests/
        pytest --cov=../agents --cov=../model_pack --cov-report=xml --cov-report=html

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./tests/coverage.xml

    - name: Run performance tests
      run: |
        cd tests/
        pytest -m performance -v

    - name: Generate quality report
      run: |
        cd tests/
        python generate_quality_report.py

    - name: Upload test artifacts
      uses: actions/upload-artifact@v3
      with:
        name: test-reports
        path: tests/test_reports/
```

## ðŸš¨ Test Monitoring and Alerting

### Test Health Dashboard
```python
def generate_test_health_dashboard():
    """Generate real-time test health dashboard"""

    dashboard_data = {
        'last_run': datetime.now().isoformat(),
        'test_suites': {},
        'quality_metrics': {},
        'alerts': []
    }

    # Check each test suite
    test_suites = [
        ('Agent System', 'test_agent_system.py'),
        ('Model Integration', 'test_model_pack_comprehensive.py'),
        ('Performance', 'test_performance.py'),
        ('Integration', 'test_integration.py')
    ]

    for suite_name, test_file in test_suites:
        result = subprocess.run([
            'pytest', test_file, '--tb=no', '-q'
        ], capture_output=True, text=True)

        dashboard_data['test_suites'][suite_name] = {
            'status': 'passed' if result.returncode == 0 else 'failed',
            'exit_code': result.returncode
        }

        if result.returncode != 0:
            dashboard_data['alerts'].append({
                'type': 'test_failure',
                'suite': suite_name,
                'message': f"Test suite {suite_name} failed",
                'timestamp': datetime.now().isoformat()
            })

    return dashboard_data
```

---

**Testing Philosophy**: This test suite ensures Script Ohio 2.0 maintains high quality standards through comprehensive unit, integration, and performance testing. The pytest-based framework provides reliable, fast feedback during development while ensuring production readiness.

**Enhanced Agent Testing**: Specialized testing for the intelligent agent system ensures role-based functionality, performance requirements, and integration with ML models are thoroughly validated.

**Quality Goals**: 90%+ code coverage, <2s response times, >95% integration success, and comprehensive error handling for robust production deployment.

**Continuous Quality**: Automated testing pipelines, performance monitoring, and quality gates ensure consistent code quality and rapid detection of regressions.