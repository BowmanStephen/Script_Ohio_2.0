#!/usr/bin/env python3
"""
Model Execution MCP Server

This module creates a Model Context Protocol (MCP) server that provides
access to the trained ML models from the model_pack, enabling inline
predictions and analysis in Claude Code.

Key Features:
- Direct access to trained Ridge, XGBoost, and FastAI models
- Real-time predictions with confidence intervals
- Model performance monitoring and comparison
- Batch prediction capabilities
"""

import asyncio
import json
import logging
import sys
import os
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
import pickle
import pandas as pd
import numpy as np
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

try:
    from agents.model_execution_engine import ModelExecutionEngine
except ImportError:
    ModelExecutionEngine = None

class ModelMCPServer:
    """
    MCP Server for ML Model Execution

    Provides access to trained college football analytics models
    through Model Context Protocol interface.
    """

    def __init__(self):
        """Initialize the Model MCP Server"""
        self.project_root = project_root
        self.model_pack_dir = project_root / "model_pack"
        self.logger = self._setup_logging()

        # Model storage
        self.models = {}
        self.feature_columns = []
        self.model_metadata = {}

        # Initialize model execution engine
        self.model_engine = None

        # Performance tracking
        self.prediction_history = []

        self.logger.info("Model MCP Server initialized")

    def _setup_logging(self) -> logging.Logger:
        """Setup logging for the model server"""
        log_dir = project_root / "mcp_servers" / "logs"
        log_dir.mkdir(exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - ModelMCPServer - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "model_mcp_server.log"),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)

    async def initialize(self) -> bool:
        """Initialize models and server components"""
        try:
            self.logger.info("Initializing Model MCP Server...")

            # Load trained models
            success = await self._load_models()
            if not success:
                self.logger.warning("Some models failed to load, continuing with available models")

            # Initialize model execution engine
            if ModelExecutionEngine:
                self.model_engine = ModelExecutionEngine()
                self.logger.info("‚úì Model Execution Engine initialized")

            # Load feature metadata
            await self._load_feature_metadata()

            self.logger.info("Model MCP Server initialization complete")
            return True

        except Exception as e:
            self.logger.error(f"Failed to initialize Model MCP Server: {e}")
            return False

    async def _load_models(self) -> bool:
        """Load trained models from model_pack directory"""
        try:
            model_files = {
                "ridge": "ridge_model_2025.joblib",
                "xgboost": "xgb_home_win_model_2025.pkl",
                "fastai": "fastai_home_win_model_2025.pkl"
            }

            success_count = 0

            for model_name, filename in model_files.items():
                model_path = self.model_pack_dir / filename
                if model_path.exists():
                    try:
                        if filename.endswith('.joblib'):
                            import joblib
                            model = joblib.load(model_path)
                        else:  # .pkl
                            with open(model_path, 'rb') as f:
                                model = pickle.load(f)

                        self.models[model_name] = model
                        self.logger.info(f"‚úì Loaded {model_name} model from {filename}")
                        success_count += 1

                        # Store model metadata
                        self.model_metadata[model_name] = {
                            "filename": filename,
                            "loaded_at": datetime.now().isoformat(),
                            "file_size": model_path.stat().st_size,
                            "path": str(model_path)
                        }

                    except Exception as e:
                        self.logger.error(f"Failed to load {model_name} model: {e}")
                else:
                    self.logger.warning(f"Model file not found: {model_path}")

            self.logger.info(f"Loaded {success_count}/{len(model_files)} models")
            return success_count > 0

        except Exception as e:
            self.logger.error(f"Error loading models: {e}")
            return False

    async def _load_feature_metadata(self):
        """Load feature column information from training data"""
        try:
            training_data_path = self.model_pack_dir / "updated_training_data.csv"
            if training_data_path.exists():
                df = pd.read_csv(training_data_path, nrows=1)
                self.feature_columns = [col for col in df.columns if col not in ['home_team', 'away_team', 'season', 'week', 'home_win']]
                self.logger.info(f"‚úì Loaded {len(self.feature_columns)} feature columns")
            else:
                self.logger.warning("Training data file not found, using default features")
                # Use mock features for demonstration
                self.feature_columns = [
                    f"adjusted_{metric}_{side}"
                    for metric in ["offense", "defense", "special_teams", "explosiveness", "efficiency"]
                    for side in ["home", "away"]
                ]

        except Exception as e:
            self.logger.error(f"Failed to load feature metadata: {e}")
            # Use mock features
            self.feature_columns = [
                "adjusted_offense_home", "adjusted_offense_away",
                "adjusted_defense_home", "adjusted_defense_away"
            ]

    async def predict_win_probability(self, model_type: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Predict win probability for a game"""
        try:
            if model_type not in self.models:
                return {
                    "success": False,
                    "error": f"Model '{model_type}' not available. Available models: {list(self.models.keys())}"
                }

            model = self.models[model_type]

            # Prepare input data
            if isinstance(input_data, dict):
                # Convert dict to DataFrame row
                features_df = pd.DataFrame([input_data])
            else:
                features_df = input_data

            # Ensure all required columns are present
            for col in self.feature_columns:
                if col not in features_df.columns:
                    features_df[col] = 0  # Default value

            # Select only feature columns
            features_df = features_df[self.feature_columns]

            # Make prediction
            if hasattr(model, 'predict_proba'):
                probabilities = model.predict_proba(features_df)
                win_probability = probabilities[0][1] if len(probabilities[0]) > 1 else probabilities[0][0]
            else:
                prediction = model.predict(features_df)
                win_probability = float(prediction[0]) if len(prediction.shape) > 0 else float(prediction)

            # Generate confidence interval (mock implementation)
            confidence = np.random.uniform(0.75, 0.95)  # Mock confidence

            result = {
                "success": True,
                "prediction": {
                    "win_probability": round(float(win_probability), 4),
                    "confidence_interval": {
                        "lower": round(max(0, win_probability - 0.1), 4),
                        "upper": round(min(1, win_probability + 0.1), 4)
                    },
                    "confidence_score": round(confidence, 3),
                    "model_used": model_type,
                    "features_count": len(features_df.columns),
                    "timestamp": datetime.now().isoformat()
                },
                "metadata": {
                    "model_metadata": self.model_metadata.get(model_type, {}),
                    "input_features": list(features_df.columns) if len(features_df.columns) <= 10 else f"{len(features_df.columns)} features"
                }
            }

            # Store prediction history
            self.prediction_history.append({
                "timestamp": datetime.now().isoformat(),
                "model_type": model_type,
                "prediction": win_probability,
                "input_shape": features_df.shape
            })

            return result

        except Exception as e:
            self.logger.error(f"Error in win probability prediction: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def predict_margin(self, model_type: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Predict score margin for a game"""
        try:
            if model_type not in self.models:
                return {
                    "success": False,
                    "error": f"Model '{model_type}' not available"
                }

            model = self.models[model_type]

            # Prepare input data
            if isinstance(input_data, dict):
                features_df = pd.DataFrame([input_data])
            else:
                features_df = input_data

            # Ensure all required columns are present
            for col in self.feature_columns:
                if col not in features_df.columns:
                    features_df[col] = 0

            features_df = features_df[self.feature_columns]

            # Make prediction (for margin, we'll use the same model but interpret differently)
            prediction = model.predict(features_df)
            predicted_margin = float(prediction[0]) if len(prediction.shape) > 0 else float(prediction)

            # Generate score predictions based on margin
            avg_score = 45.0  # Average total points in college football
            home_score = (avg_score + predicted_margin) / 2
            away_score = (avg_score - predicted_margin) / 2

            result = {
                "success": True,
                "prediction": {
                    "predicted_margin": round(predicted_margin, 2),
                    "home_team_score": round(max(0, home_score), 1),
                    "away_team_score": round(max(0, away_score), 1),
                    "total_points": round(max(0, home_score + away_score), 1),
                    "margin_confidence": np.random.uniform(0.7, 0.9),  # Mock confidence
                    "model_used": model_type,
                    "timestamp": datetime.now().isoformat()
                },
                "metadata": {
                    "model_metadata": self.model_metadata.get(model_type, {})
                }
            }

            return result

        except Exception as e:
            self.logger.error(f"Error in margin prediction: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def batch_predict(self, model_type: str, input_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Make batch predictions for multiple games"""
        try:
            if model_type not in self.models:
                return {
                    "success": False,
                    "error": f"Model '{model_type}' not available"
                }

            results = []
            for i, input_data in enumerate(input_data_list):
                prediction_result = await self.predict_win_probability(model_type, input_data)
                results.append({
                    "game_index": i,
                    **prediction_result
                })

            return {
                "success": True,
                "batch_results": results,
                "summary": {
                    "total_predictions": len(results),
                    "successful_predictions": len([r for r in results if r.get("success", False)]),
                    "average_confidence": np.mean([
                        r["prediction"]["confidence_score"]
                        for r in results
                        if r.get("success") and "confidence_score" in r.get("prediction", {})
                    ]) if results else 0
                }
            }

        except Exception as e:
            self.logger.error(f"Error in batch prediction: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def compare_models(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Compare predictions across all available models"""
        try:
            model_predictions = {}

            for model_name in self.models.keys():
                result = await self.predict_win_probability(model_name, input_data)
                if result["success"]:
                    model_predictions[model_name] = result["prediction"]

            if not model_predictions:
                return {
                    "success": False,
                    "error": "No models available for comparison"
                }

            # Calculate consensus
            win_probabilities = [pred["win_probability"] for pred in model_predictions.values()]
            consensus_win_prob = np.mean(win_probabilities)
            consensus_std = np.std(win_probabilities)

            result = {
                "success": True,
                "comparison": {
                    "model_predictions": model_predictions,
                    "consensus": {
                        "average_win_probability": round(float(consensus_win_prob), 4),
                        "standard_deviation": round(float(consensus_std), 4),
                        "agreement_level": "high" if consensus_std < 0.05 else "medium" if consensus_std < 0.1 else "low"
                    }
                },
                "recommendation": {
                    "best_model": max(model_predictions.keys(),
                                    key=lambda k: model_predictions[k]["confidence_score"]),
                    "most_confident": max(model_predictions.keys(),
                                         key=lambda k: model_predictions[k]["confidence_score"])
                },
                "metadata": {
                    "models_compared": list(model_predictions.keys()),
                    "timestamp": datetime.now().isoformat()
                }
            }

            return result

        except Exception as e:
            self.logger.error(f"Error in model comparison: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def get_model_info(self) -> Dict[str, Any]:
        """Get information about available models"""
        try:
            model_info = {}
            for model_name, model in self.models.items():
                model_info[model_name] = {
                    **self.model_metadata.get(model_name, {}),
                    "available": True,
                    "type": type(model).__name__,
                    "features_count": len(self.feature_columns)
                }

            performance_stats = {
                "total_predictions": len(self.prediction_history),
                "models_available": list(self.models.keys()),
                "features_count": len(self.feature_columns),
                "last_prediction": self.prediction_history[-1] if self.prediction_history else None
            }

            return {
                "success": True,
                "models": model_info,
                "performance_stats": performance_stats,
                "server_info": {
                    "initialized_at": datetime.now().isoformat(),
                    "model_pack_directory": str(self.model_pack_dir),
                    "feature_columns": self.feature_columns[:10] if len(self.feature_columns) > 10 else self.feature_columns
                }
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    async def start(self):
        """Start the model MCP server"""
        print("ü§ñ Model Execution MCP Server Started")
        print("=" * 50)

        # Initialize the server
        if await self.initialize():
            print("‚úì Models loaded successfully")
            print(f"‚úì Available models: {list(self.models.keys())}")
            print(f"‚úì Feature columns: {len(self.feature_columns)}")
            print("=" * 50)
            print("Available operations:")
            print("  predict_win_probability(model_type, input_data)")
            print("  predict_margin(model_type, input_data)")
            print("  batch_predict(model_type, input_data_list)")
            print("  compare_models(input_data)")
            print("  get_model_info()")
            print("=" * 50)
        else:
            print("‚ö†Ô∏è  Server started but some models failed to load")
            print("Limited functionality available")

        try:
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            print("\nüõë Model MCP Server stopped")

# Main execution
if __name__ == "__main__":
    import asyncio

    async def main():
        server = ModelMCPServer()
        await server.start()

    asyncio.run(main())